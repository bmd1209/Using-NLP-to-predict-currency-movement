{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd96b696-f261-4e16-8bb9-34349063576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import OpenBlender\n",
    "import json\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "from pathlib import Path\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d857a9-2d52-481b-bd9e-969adcd9ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# import these modules \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.sentiment.vader\n",
    "\n",
    "ps = PorterStemmer() # stemming\n",
    "wordnet_lemmatizer = WordNetLemmatizer()  #Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4bf73e-2b35-4ef0-814d-65f44e3cc0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentiment score applying the vaderSentiment functions\n",
    "file_path = Path('5ea20b4e95162936348f141d.csv')\n",
    "initial_lite = pd.read_csv(file_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf11e91-d5d1-4a3d-9304-134849c78b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lite.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b65eca3-3732-4bfa-be07-d8adf472879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lite.drop_duplicates('timestamp', inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8af5d6-1f79-4725-9a24-e5481f9d64e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "#combine sad and happy emoticons\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee6c323-604d-4a98-805a-854178262ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emoticons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f04af-fda0-4c02-af48-7bf32ceb83f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Emojis pattern\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "    \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a13389-678c-41be-894b-5f20f4da09b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3e7a7-fb0b-471c-b761-d84d71ce73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuations pattern\n",
    "punctuation_pattern = string.punctuation + \"▼\"\n",
    "punctuation_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b80b5-c010-45e3-9673-ca80d8325422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords_re = set(STOPWORDS)\n",
    "update_list = [\"btc\", \"bitcoin\",\"blockchain\", \"crypto\",\"cryptocurrency\",\"ha\",\"say\",\"time\",\"want\",\"gonna\",\"use\",\"a\",\"about\",\n",
    "              \"actually\",\"almost\",\"also\",\"although\",\"always\",\"am\",\"an\",\"and\",\"any\",\"are\",\"as\",\"at\",\"be\",\"became\",\"become\",\n",
    "              \"but\",\"by\",\"can\",\"could\",\"did\",\"do\",\"does\",\"each\",\"either\",\"else\",\"for\",\"from\",\"had\",\"has\",\"have\",\"hence\",\"how\",\n",
    "              \"I\",\"if\",\"in\",\"is\",\"its\",\"just\",\"may\",\"maybe\",\"me\",\"might\",\"mine\",\"must\",\"my\",\"mine\",\"must\",\"neither\",\"nor\",\"not\",\n",
    "              \"of\",\"oh\",\"ok\",\"when\",\"where\",\"whereas\",\"wherever\",\"whenever\",\"whether\",\"which\",\"while\",\"who\",\"whom\",\"whomever\",\"whose\",\n",
    "              \"why\",\"will\",\"with\",\"within\",\"without\",\"would\",\"yes\",\"yet\",\"you\",\"your\",\"people\"]\n",
    "stopwords_re.update(update_list)\n",
    "print(stopwords_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ce9b8-3c67-495d-8f66-bf8a7604df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "clean_tweets()\n",
    "contains: remove links, tokenize, reomve non-ASCII chars, convert to lower case, remove stop_words, remove\n",
    "'''\n",
    "def clean_tweets(tweet):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # remove links\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    #after tweepy preprocessing the colon left remain after removing mentions\n",
    "    #or RT sign in the import re\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "    tweet = re.sub(r'([0-9])\\w+','',tweet)\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    #remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    #tweet = re.sub(r\"\\b\\w+\\b\",' ', tweet)\n",
    "    #filter using NLTK library append it to a string\n",
    "    #filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_tweet = []\n",
    "    #looping through conditions\n",
    "    for w in word_tokens:\n",
    "        # convert to lower case\n",
    "        w = w.lower()\n",
    "        # lemmanization\n",
    "        w = wordnet_lemmatizer.lemmatize(w)\n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in emoticons and w not in punctuation_pattern:\n",
    "            filtered_tweet.append(w)\n",
    "    return ' '.join(filtered_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da038609-1365-46ef-b281-f780e5b07667",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lite['clean_text']=initial_lite['VEC.COINJOURNA.text_last1days:bitcoin'].apply(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417ccdcb-b359-48d2-987e-2f816c0ac052",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path('Lit_prices_target.csv')\n",
    "df_anchor_lite=pd.read_csv(file_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3afdfb3-c539-4900-bb37-8a286d9735c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anchor = pd.concat([df_anchor_lite, initial_lite])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0688d3-f8dd-47e0-9e0e-c60a623b9376",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b44430-eec7-428e-a662-5bbeb6dfe066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from imageio import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:crypto-anal] *",
   "language": "python",
   "name": "conda-env-crypto-anal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
